
\section{Other topics}
\label{section:more}
\setcounter{footnote}{0}

\subsection{How do I cite HMMER?}

The appropriate citation is to the web site, \url{hmmer.org}. You
should also cite what version of the software you used. We archive all
old versions, so anyone should be able to obtain the version you used,
when exact reproducibility of an analysis is an issue. 

The version number is in the header of most output files. To see it
quickly, do something like \prog{hmmscan -h} to get a help page, and
the header will say:

\begin{sreoutput}
# hmmscan :: search sequence(s) against a profile database
# HMMER 3.0 (March 2010); http://hmmer.org/
# Copyright (C) 2010 Howard Hughes Medical Institute.
# Freely distributed under the GNU General Public License (GPLv3).
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\end{sreoutput}

So (from the second line there) this is from HMMER 3.0.

There is not yet any appropriate citable published paper that
describes the HMMER3 software suite.


\subsection{How do I report a bug?}

Email us, at \url{hmmer@janelia.hhmi.org}.

Before we can see what needs fixing, we almost always need to
reproduce a bug on one of our machines. This means we want to have a
small, reproducible test case that shows us the failure you're seeing.
So if you're reporting a bug, please send us:

\begin{itemize}
 \item A brief description of what went wrong.
 \item The command line(s) that reproduce the problem.
 \item Copies of any files we need to run those command lines.
 \item Information about what kind of hardware you're on, what
   operating system, and (if you compiled the software yourself rather
   than running precompiled binaries), what compiler and version you
   used, with what configuration arguments.
\end{itemize}

Depending on how glaring the bug is, we may not need all this
information, but any work you can put into giving us a clean
reproducible test case doesn't hurt and often helps.

The information about hardware, operating system, and compiler is
important. Bugs are frequently specific to particular configurations
of hardware/OS/compiler.  We have a wide variety of systems available
for trying to reproduce bugs, and we'll try to match your system as
closely as we can.

If you first see a problem on some huge compute (like running a
zillion query sequence over a huge profile database), it will really,
really help us if you spend a bit of time yourself trying to isolate
whether the problem really only manifests itself on that huge compute,
or if you can isolate a smaller test case for us. The ideal bug report
(for us) gives us everything we need to reproduce your problem in one
email with at most some small attachments. 

Remember, we're not a company with dedicated support staff -- we're a
small lab of busy researchers like you. Somebody here is going to drop
what they're doing to try to help you out. Meet us halfway, try to
save us some time, and we're more likely to stay in our usual good
mood.

If we're in our usual good mood, we'll reply quickly. We'll probably
tell you we fixed the bug in our development code, and that the fix
will appear in the next HMMER release. This of course doesn't help you
much, since nobody knows when the next HMMER release is going to be.
So if possible, we'll usually try to describe a workaround for the
bug. 

If the code fix is small, we might also tell you how to patch and
recompile the code yourself. You may or may not want to do this.

There are currently not enough open bugs to justify having a formal
on-line bug tracking system. We have a bugtracking system, but it's
internal.




\subsection{Input files}

\subsubsection{Reading from a stdin pipe using - (dash) as a filename argument}

Generally, HMMER programs read their sequence and/or profile input
from files. Unix power users often find it convenient to string an
incantation of commands together with pipes (indeed, such wizardly
incantations are a point of pride). For example, you might extract a
subset of query sequences from a larger file using a one-liner
combination of scripting commands (perl, awk, whatever). To facilitate
the use of HMMER programs in such incantations, you can almost always
use an argument of '-' (dash) in place of a filename, and the program
will take its input from a standard input pipe instead of opening a
file.

For example, the following three commands are entirely equivalent, and
give essentially identical output:

\user{hmmsearch globins4.hmm uniprot\_sprot.fasta} 

\user{cat globins4.hmm | hmmsearch - uniprot\_sprot.fasta}

\user{cat uniprot\_sprot.fasta | hmmsearch globins4.hmm - }

Most Easel ``miniapp'' programs share the same ability of pipe-reading.

Because the programs for profile HMM fetching (\prog{hmmfetch}) and
sequence fetching (\prog{esl-sfetch}) can fetch any number of profiles
or sequences by names/accessions given in a list, \emph{and} these
programs can also read these lists from a stdin pipe, you can craft
incantations that generate subsets of queries or targets on the
fly. For example:

\user{esl-sfetch --index uniprot\_sprot.fasta}
\user{cat mytargets.list | esl-sfetch -f uniprot\_sprot.fasta - | hmmsearch globins4.hmm -}

This takes a list of sequence names/accessions in
\prog{mytargets.list}, fetches them one by one from Uniprot (note that
we index the Uniprot file first, for fast retrieval; and note that
\prog{esl-sfetch} is reading its \prog{<namefile>} list of
names/accessions through a pipe using the '-' argument), and pipes
them to an \prog{hmmsearch}. It should be obvious from this that we
can replace the \prog{cat mytargets.list} with \emph{any} incantation
that generates a list of sequence names/accessions (including SQL
database queries).

Ditto for piping subsets of profiles. Supposing you have a copy of Pfam in Pfam-A.hmm:

\user{hmmfetch --index Pfam-A.hmm}
\user{cat myqueries.list | hmmfetch -f Pfam.hmm - | hmmsearch - uniprot\_sprot.fasta}

This takes a list of query profile names/accessions in
\prog{myqueries.list}, fetches them one by one from Pfam, and does an
hmmsearch with each of them against Uniprot. As above, the \prog{cat
  myqueries.list} part can be replaced by any suitable incantation
that generates a list of profile names/accessions.

There are three kinds of cases where using '-' is restricted or
doesn't work. A fairly obvious restriction is that you can only use
one '-' per command; you can't do a \prog{hmmsearch - -} that tries to
read both profile queries and sequence targets through the same stdin
pipe. Second, another case is when an input file must be obligately
associated with additional, separately generated auxiliary files, so
reading data from a single stream using '-' doesn't work because the
auxiliary files aren't present (in this case, using '-' will be
prohibited by the program). An example is \prog{hmmscan}, which needs
its \prog{<hmmfile>} argument to be associated with four auxiliary
files named \prog{<hmmfile>.h3\{mifp\}} that \prog{hmmpress} creates,
so \prog{hmmscan} does not permit a '-' for its \prog{<hmmfile>}
argument. Finally, when a command would require multiple passes over
an input file, the command will generally abort after the first pass
if you are trying to read that file through a standard input pipe
(pipes are nonrewindable in general; a few HMMER or Easel programs
will buffer input streams to make multiple passes possible, but this
is not usually the case). An example would be trying to search a file
containing multiple profile queries against a streamed target
database:

\user{cat myqueries.list | hmmfetch -f Pfam.hmm > many.hmms}
\user{cat mytargets.list | esl-sfetch -f uniprot\_sprot.fasta - | hmmsearch many.hmms -}

This will fail. Unfortunately the above business about how it will
``generally abort after the first pass'' means it fails weirdly. The
first query profile search will succeed, and its output will appear;
then an error message will be generated when \prog{hmmsearch} sees the
\emph{second} profile query and oops, it realizes it is unable to
rewind the target sequence database stream. This is inherent in how it
reads the profile HMM query file sequentially as a stream (which is
what's allowing it to read input from stdin pipes in the first place),
one model at a time: it doesn't see there's more than one query model
in the file until it gets to the second model.

This case isn't too restricting because the same end goal can be
achieved by reordering the commands. In cases where you want to do
multiple queries against multiple targets, you always want to be
reading the \emph{queries} from a stdin pipe, not the targets:

\user{cat mytargets.list | esl-sfetch -f uniprot\_sprot.fasta > mytarget.seqs}
\user{cat myqueries.list | hmmfetch -f Pfam.hmm - |  hmmsearch - mytarget.seqs}

So in this multiple queries/multiple targets case of using stdin
pipes, you just have to know, for any given program, which file it
considers to be queries and which it considers to be targets. (That
is, the logic in searching many queries against many targets is ``For
each query: search the target database; then rewind the target
database to the beginning.'') For \prog{hmmsearch}, the profiles are
queries and sequences are targets. For \prog{hmmscan}, the reverse.

In general, HMMER and Easel programs document in their man page
whether (and which) command line arguments can be replaced by '-'.
You can always check by trial and error, too. The worst that can
happen is a ``Failed to open file -'' error message, if the program
can't read from pipes.



\subsection{Null hypotheses in play}

\subsubsection{null1 model}

\subsubsection{biased composition model}

\subsubsection{null2 model}


\subsection{Profile scoring algorithms and implementations}

\subsubsection{MSV filter}

\subsubsection{Viterbi filter}

\subsubsection{Forward filter/parser}

\subsubsection{Backward parser}





\subsection{The steps in the target processing pipeline}

One comparison: one sequence to one HMM.

\begin{description}

\item[\textbf{Null1 model score.}]  

  The ``null1 model'' is a probabilistic model for the null hypothesis
  that the target sequence is not homologous to the query profile. The
  null model is a one-state HMM configured to generate ``random''
  sequences of the same mean length $L$ as the target sequence, with
  each residue drawn from a background frequency distribution (a
  standard i.i.d. model: residues are treated as independent and
  identically distributed). Currently, this background frequency
  distribution is hardcoded as the mean residue frequencies in
  Swissprot 50.8 (October 2006).

\item[\textbf{MSV Filter.}] 

 The sequence is aligned to the profile using a specialized model that
 allows multiple high-scoring local ungapped segments to match.  The
 optimal alignment score (Viterbi score) is calculated under this
 multisegment model, hence the term MSV, for ``multi-segment
 Viterbi''. This is HMMER's main speed heuristic.

 The MSV score is comparable to BLAST's sum score (optimal sum of
 ungapped alignment segments). HMMER calculates this score directly
 (in vectorized dynamic programming) without additional heuristics
 (such as BLAST's word hit and hit extension stages). Also, the MSV
 score is a true log-odds likelihood ratio, so it obeys conjectures
 about the expected score distribution \citep{Eddy08} that allow
 immediate and accurate calculation of the statistical significance
 (P-value) of the score.

 By default, comparisons with a P-value of $\leq$ 0.02 pass this
 filter, meaning that about $2\%$ of the comparisons pass. You can use
 the \ccode{--F1 <x>} option to change this threshold. For example,
 \ccode{--F1 <0.05>} would pass 5\% of the comparisons, making a
 search more sensitive but slower. Setting the threshold to $>1.0$
 (\ccode{--F1 99} for example) assures that all comparisons will
 pass. Shutting off the MSV filter may be worthwhile if you want to
 make sure you don't miss comparisons that have a lot of scattered
 insertions and deletions. Alternatively, the \ccode{--max} option
 causes the MSV filter step (and all other filter steps) to be
 bypassed.

 The MSV bit score is calculated as a log-odds score using the null1
 model for comparison. No correction for a biased composition or
 repetitive sequence is done at this stage. For comparisons involving
 biased sequences and/or profiles, more than 2\% of comparisons will
 pass the MSV filter. At the end of search output, there is a line
 like:

\begin{sreoutput}
 Passed MSV filter:                    107917  (0.020272); expected 106468.8 (0.02)
\end{sreoutput}

 which tells you how many and what fraction of comparisons passed the
 MSV filter, versus how many (and what fraction) were expected. 

\item[\textbf{Biased composition filter.}]
 Unfortunately some searches involving biased sequence/profile
 comparisons allow so many comparisons through the MSV filter that
 HMMER's speed performance is severely degraded. Although the final
 scores and E-values at the end of the pipeline will be calculated
 taking into account a ``null2'' model of biased composition and
 simple repetition, the null2 model is dependent on a full alignment
 ensemble calculation via the Forward/Backward algorithm, making it
 computationally complex, so it won't get calculated until the very
 end. The treatment of biased composition comparisons is probably the
 most serious problem remaining in HMMER3, and it will require more
 research. As a stopgap solution to rescuing most of the speed
 degradation while not sacrificing too much sensitivity, an \emph{ad
   hoc} biased composition filtering step is applied to remove highly
 biased comparisons.

 On the fly, a two-state HMM is constructed. One state emits residues
 from the background frequency distribution (same as the null1 model),
 and the other state emits residues from the mean residue composition
 of the profile (i.e. the expected composition of sequences generated
 by the core model, including match and insert states
 [\ccode{p7\_hmm.c:p7\_hmm\_SetComposition()}]). Thus if the profile
 is highly biased (cysteine-rich, for example; or highly hydrophobic
 with many transmembrane segments), this composition bias will be
 captured by this second state. This model's transitions are
 arbitrarily set such that state 1 emits an expected length of 400 at
 a time, and state 2 emits an expected length of M/8 at a time (for a
 profile of length M). An overall target sequence length distribution
 is set to a mean of $L$, identical to the null1 model. 

 The sequence is then rescored using this ``bias filter model'' in
 place of the null1 model, using the HMM Forward algorithm. (This
 replaces the null1 model score at all subsequent filter steps in the
 pipeline, until a final Forward score is calculated.) A new MSV bit
 score is obtained.

 If the P-value of this still satisfies the MSV thresholds, the
 sequence passes the biased composition filter. 

 The \ccode{--F1 <x>} option controls the P-value threshold for
 passing the MSV filter score, both before (with the simple null1
 model) and after the bias composition filter is applied.

 The \ccode{--max} option bypasses all filters in the pipeline,
 including the bias filter.

 The \ccode{--nobias} option turns off (bypasses) the biased
 composition filter.  The simple null1 model is used as a null
 hypothesis for MSV and in subsequent filter steps. The biased
 composition filter step compromises a small amount of sensitivity,
 and though it is good to have it on by default, you may want to shut
 it off if you know you will have no problem with biased composition
 hits.

 At the end of a search output, you will see a line like:

\begin{sreoutput}
 Passed bias filter:                   105665  (0.019849); expected 106468.8 (0.02)
\end{sreoutput}

 which tells you how many and what fraction of comparisons passed the
 biased composition filter, versus how many were expected. (If the
 filter was turned off, all comparisons pass.)

\item[\textbf{Viterbi filter.}]
 The sequence is now aligned to the profile using a fast Viterbi
 (optimal alignment) algorithm. 

 This Viterbi implementation is specialized for speed in several
 respects. It is implemented in 16-way parallel SIMD vector
 instructions, using reduced precision scores that have been scaled to
 8-bit integers (range 0..255). Reduced precision introduces a score
 jitter of about $\pm 1$ bit. Only one row of the dynamic programming
 matrix is stored, so the routine only recovers a score, not an
 optimal alignment. The representation has limited range; local
 alignment scores do not underflow, but high scoring comparisons will
 overflow and return infinity when they exceed about 20 bits; all
 score overflows automatically pass the filter.

 The Viterbi filter bit score is then computed using the appropriate
 null model log likelihood (the biased composition filter model score,
 or if the biased filter is off, the null1 model score).

 
 
 
 







 


 
 


  

\item[\textbf{Viterbi filter.}]

\item[\textbf{Forward score.}]

\item[\textbf{Backward score.}]

\item[\textbf{Domain definition.}]

\item[\textbf{Null2 model score applied.}]

\item[\textbf{Storage of significant hits.}]
\end{description}



\begin{description}
\item[\textbf{Posterior decoding.}]

\item[\textbf{Region identification.}]

\item[\textbf{Test for multidomain regions.}]

\item[\textbf{Resolve multidomain regions by clustering of alignment ensemble.}]

\item[\textbf{Domain envelope definition.}]

\item[\textbf{Rescore each envelope.}]
\end{description}


\begin{description}



\subsection{Tabular output formats}

\subsubsection{The target hits table}

The target hits table consists of one line for each different
query/target comparison that met the reporting thresholds, ranked by
decreasing statistical significance (increasing E-value).  Each line
consists of \textbf{16 space-delimited fields} followed by a free text
target sequence description, as follows:

\begin{description}
\item[\emprog{(1) target:}]
  The name of the target sequence or
  profile.  (If a \ccode{--acc} option was used, and an accession is
  available for the sequence or profile, the accession is shown
  instead of the name.) 

\item[\emprog{(2) query:}] 
  The name of the query sequence or profile.
  (If a \ccode{--acc} option was used, and an accession is
  available for the sequence or profile, the accession is shown
  instead of the name.) 

\item[\emprog{(3) E-value (full sequence):}] The expectation value
  (statistical significance) of the comparison.  This is a \emph{per
  query} E-value; i.e.\ calculated as the expected number of false
  positives achieving this comparison's score for a \emph{single}
  query against the $Z$ sequences in the target dataset.  If you
  search with multiple queries and if you want to control the
  \emph{overall} false positive rate of that search rather than the
  false positive rate per query, you will want to multiply this
  per-query E-value by how many queries you're doing.

\item[\emprog{(4) score (full sequence):}] 
  The score (in bits) for this target/query comparison. It includes
  a biased-composition correction (the ``null2'' model). 

\item[\emprog{(5) Bias (full sequence):}] The biased-composition
  correction: the bit score difference contributed by the null2
  model. High bias scores may be a red flag for a false positive,
  especially when the bias score is as large or larger than the
  overall bit score. It is difficult to correct for all possible ways
  in which a nonrandom but nonhomologous biological sequences can
  appear to be similar, such as short-period tandem repeats, so there
  are cases where the bias correction is not strong enough (creating
  false positives).

\item[\emprog{(6) E-value (best 1 domain):}] 

\item[\emprog{(7) score (best 1 domain):}] 

\item[\emprog{(8) bias (best 1 domain):}] 

\item[\emprog{(9) exp:}] Expected number of domains.

\item[\emprog{(10) reg:}] Number of regions defined.

\item[\emprog{(11) clu:}]  
  Number of regions analyzed by stochastic traceback and clustering.

\item[\emprog{(12) ov:}] 
  Number of overlaps.

\item[\emprog{(13) env:}] 
  Number of envelopes defined.

\item[\emprog{(14) dom:}] 
  Number of domains defined.

\item[\emprog{(15) rep:}] 
  Number of domains satisfying reporting thresholds.

\item[\emprog{(16) inc:}] 
  Number of domains satisfying inclusion thresholds.

\item[\emprog{(17) description of target:}] 
\end{description}

This table is columnated neatly for human readability, but do not
write parsers that rely on this columnation; rely on space-delimited
fields. The pretty columnation assumes fixed maximum widths for each
field. If a field exceeds its allotted width, it will still be fully
represented and space-delimited, but the columnation will be disrupted
on the rest of the row.









\subsubsection{Domain hits table}

The domain table has \textbf{20 whitespace-delimited fields} followed
by a free text target sequence description, as follows:

\begin{description}
\item[\emprog{(1) target:}] The name of the target sequence or
  profile.  (If a \ccode{--acc} option was used, and an accession is
  available for the sequence or profile, the accession is shown
  instead of the name.) 

\item[\emprog{(2) tlen:}]

\item[\emprog{(3) query:}]

\item[\emprog{(4) qlen:}]

\item[\emprog{(5) E-value:}]

\item[\emprog{(6) score:}]

\item[\emprog{(7) bias:}]

\item[\emprog{(8) \#:}]

\item[\emprog{(9) of:}]

\item[\emprog{(10) c-Evalue:}]

\item[\emprog{(11) i-Evalue:}]

\item[\emprog{(12) score:}]

\item[\emprog{(13) bias:}]

\item[\emprog{(14) from (hmm coord):}]

\item[\emprog{(15) to (hmm coord):}]

\item[\emprog{(16) from (ali coord):}]

\item[\emprog{(17) to (ali coord):}]

\item[\emprog{(18) from (env coord):}]

\item[\emprog{(19) to (env coord):}]

\item[\emprog{(20) acc:}]

\item[\emprog{(21) description of target:}]
\end{description}

As with the target hits table (above), this table is columnated neatly
for human readability, but you should not write parsers that rely on
this columnation; parse based on space-delimited fields instead.
