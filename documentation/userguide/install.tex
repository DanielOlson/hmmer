\section{Installation}
\label{section:installation}
\setcounter{footnote}{0}

\subsection{Quick installation instructions}

Download \prog{hmmer-3.2.tar.gz} from \url{http://hmmer.org/}
or directly from 
\url{http://eddylab.org/software/hmmer3/3.2/hmmer-3.2.tar.gz}, and
unpack it:

\user{wget http://eddylab.org/software/hmmer3/3.2/hmmer-3.2.tar.gz}\\
\user{tar xf hmmer-3.2.tar.gz}\\
\user{cd hmmer-3.2}

If you have downloaded one of our packages with pre-compiled binaries
instead of source alone, they are found in the directory
\prog{./binaries}. You may copy these into a directory of your
choosing, or simply run them from here (for example, by adding this
directory to your \prog{PATH} variable).

If you have downloaded the HMMER source code, you'll need to compile the software
with configure and make:

\user{./configure}\\ 
\user{make}

To compile and run a test suite to make sure all is well, you can
optionally do:

\user{make check}

All these tests should pass.

You don't have to install HMMER programs to run them. The newly
compiled binaries are now in the \prog{src} directory. You can run
them from there. To install the programs and man pages somewhere on
your system, do:

\user{make install} 

By default, programs are installed in \prog{/usr/local/bin} and man
pages in \prog{/usr/local/share/man/man1/}. You can change the
\prog{/usr/local} prefix to any directory you want using the
\prog{./configure --prefix} option, as in \prog{./configure --prefix
  /the/directory/you/want}.

Optionally, you can install the Easel library package as well,
including its various ``miniapplications'' that are in
\prog{easel/miniapps}, in addition to its library and header files. We
don't do this by default, in case you already have a copy of Easel
separately installed. To install Easel too:

\user{cd easel; make install} 

That's it.  You can keep reading if you want to know more about
customizing a HMMER installation, or you can skip ahead to the next
chapter, the tutorial.

\subsection{System requirements}

\paragraph{Operating system:} HMMER is designed to run on
POSIX-compatible platforms, including UNIX, Linux and Apple OS/X. The
POSIX standard essentially includes all operating systems except
Microsoft Windows.\footnote{Windows 10 includes a unix-compatible
  command shell, and should work fine. For older Windows, there are
  add-on products available for making Windows more POSIX-compliant
  and more compatible with GNU-ish configures and builds. One such
  product is Cygwin, \url{http:www.cygwin.com}, which is freely
  available.}  We have tested most extensively on Intel/Linux and on
Apple OS/X.


\paragraph{Processor:} HMMER depends on vector parallelization methods
that are supported on most modern processors. H3 requires either an
x86-compatible (IA32, IA64, or Intel64) processor that supports the
SSE2 vector instruction set, or a big-endian (AIX, not Linux) PowerPC
processor that supports the Altivec/VMX instruction set. SSE2 is
supported on Intel processors from Pentium 4 on, and AMD processors
from K8 (Athlon 64) on; we believe this includes almost all Intel
processors since 2000 and AMD processors since 2003. Altivec/VMX is
supported on Motorola G4, IBM G5, and IBM PowerPC processors starting
with the Power6, which we believe includes almost all PowerPC-based
desktop systems since 1999 and servers since 2007.\footnote{
If your platform does not support one of these vector instruction
sets, the configure script will revert to an unoptimized
implementation called the ``dummy'' implementation. \textbf{The dummy
  implementation is two orders of magnitude slower.} It will enable
you to see H3's scientific features on a much wider range of
processors, but \textbf{is not suited for production work}.}


\paragraph{Compiler:} The source code is C, conforming to POSIX and ANSI
C99 standards. It should compile with any ANSI C99 compliant compiler,
including the GNU C compiler \prog{gcc}. We test the code using the
GNU \prog{gcc}, Apple \prog{llvm/clang}, and Intel \prog{icc}
compilers.


\paragraph{Libraries and other installation requirements:} HMMER includes
a software library called \href{http://bioeasel.org}{Easel}, which it
will automatically compile during its installation process.  By
default, HMMER3 does not require any additional libraries to be
installed by you, other than standard ANSI C99 libraries that should
already be present on a system that can compile C code. Bundling Easel
instead of making it a separate installation requirement is a
deliberate design decision to simplify the installation
process.\footnote{If you follow the optional installation instructions
  for Easel after you've built HMMER, and you're installing both HMMER
  and Infernal, only install Easel once, to avoid having multiple
  instantiations lying around. Though we loosely call it a
  ``library'', the Easel API is not yet stable enough to decouple it
  from the applications that use it. Alas.}

Our configuration and compilation use several UNIX utilities. Although
these utilities are \emph{supposed} to be available on all
POSIX-compliant systems, occasionally we encounter old crufty versions
that do not support all the features the \ccode{./configure} script
and Makefiles are hoping to find. We aim to build on anything, even
old Ebay'ed junk, but if you have an old system, you may want to hedge
your bets and install up-to-date versions of GNU command line tools
such as GNU make and GNU grep.

Compiling the User Guide itself (this document) requires additional
tools to be installed, including \prog{rman} and \prog{xelatex}. We
provide all the source .tex files in case they're useful, but we do
\textbf{not} expect you to be able to compile the User Guide from
source, and we have not invested enough effort to make our process
portable.  Please don't hassle us about it (looking at you, Debian
Linux) unless you're volunteering.


\subsection{Multithreaded parallelization for multicores is default}

HMMER supports multicore parallelization using POSIX threads. By
default, the configure script will identify whether your platform
supports POSIX threads (almost all platforms do), and will
automatically compile in multithreading support.

If you want to disable multithreading at compile time, compile from
source with the \ccode{--disable-threads} flag to \ccode{./configure}.

Multithreaded HMMER programs use master/worker parallelization, with
\ccode{<n>} worker threads and one master thread. When HMMER is run on
a machine with multiple available cores, the default number of worker
threads is two\footnote{Set by a compile-time configuration option,
  \ccode{P7\_NCPU}, in \ccode{p7\_config.h.in}.}. You can control the
number of cores each HMMER process will use for computation with the
\ccode{--cpu <n>} command line option or the \ccode{HMMER\_NCPU}
environment variable.

If you specify \ccode{--cpu 0}, a HMMER search program will run in
serial-only mode, with no threads. This might be useful if you suspect
something is awry with the threaded parallel implementation.  Even
with a single worker thread (\ccode{--cpu 1}), HMMER will be faster
than serial-only mode, because the master thread handles input and
output.

If you are running HMMER on a cluster that enforces policy on the
number of cores a process can use, you need to count both the workers
and the master: you want to tell your cluster management software that
HMMER needs \ccode{<n>}+1 cores.


\subsection{MPI parallelization for clusters is optional}

MPI (Message Passing Interface) parallelization on clusters is now
supported in hmmbuild and all search programs except nhmmer and 
nhmmscan. To use MPI, you first need to have an MPI library installed, 
such as OpenMPI (\url{www.open-mpi.org}). 

MPI support is not enabled by default, and it is not compiled into the
precompiled binaries that we supply with HMMER. To enable MPI support
at compile time, give the \ccode{--enable-mpi} option to the
\ccode{./configure} command.

To use MPI parallelization, each program that has an MPI-parallel mode
has an \ccode{--mpi} command line option. This option activates a
master/worker parallelization mode. (Without the \ccode{--mpi} option,
if you run a program under \ccode{mpirun} on N nodes, you'll be
running N independent duplicate commands, not a single MPI-enabled
command. Don't do that.)

The MPI implementation for \prog{hmmbuild} scales well up to hundreds
of processors, and \prog{hmmsearch} scales all right. The other search
programs (\prog{hmmscan}, \prog{phmmer}, and \prog{jackhmmer}) scale
poorly, and probably shouldn't be used on more than tens of processors
at most. Improving MPI scaling is one of our goals.


\subsection{Using build directories}

The configuration and compilation process from source supports using
separate build trees, using the GNU-standard VPATH mechanism. This
allows you to maintain separate builds for different processors or
with different configuration/compilation options. All you have to do
is run the configure script from the directory you want to be the root
of your build tree.  For example:

\user{mkdir my-hmmer-build}\\
\user{cd my-hmmer-build}\\
\user{../configure}\\
\user{make}

This assumes you have a \ccode{make} that supports VPATH. If your
system's \ccode{make} does not, you can install GNU make.

You have to choose either to compile in one or more build tree(s), or
to compile in the source tree (i.e. the top-level HMMER directory
where you unpacked the tarball); you can't mix.  If you compile in the
source tree, and then you decide to use a build tree too, do a
\ccode{make clean} first in the top-level source
directory.\footnote{You really shouldn't have to do this, but we have
  some sort of glitch in our configure script and/or Makefiles that is
  getting in the way.}


\subsection{Makefile targets}

\begin{sreitems}{\emprog{distclean}}

\item[\emprog{all}]
  Builds everything. Same as just saying \ccode{make}.

\item[\emprog{check}]
  Runs automated test suites in both HMMER and the Easel library.

\item[\emprog{clean}]
  Removes all files generated by compilation (by
  \ccode{make}). Configuration (files generated by
  \ccode{./configure}) is preserved.

\item[\emprog{distclean}]
  Removes all files generated by configuration (by \ccode{./configure})
  and by compilation (by \ccode{make}). 

  Note that if you want to make a new configuration (for example, to
  try an MPI version by \ccode{./configure --enable-mpi; make}) you
  should do a \ccode{make distclean} (rather than a \ccode{make
    clean}), to be sure old configuration files aren't used
  accidentally.
\end{sreitems}


\subsection{Seeing more output from \prog{make}}

By default, our \prog{make} hides what's really going on with the
compilation with a pretty wrapper.  If you want to see what the
command lines really look like in all their ugly glory, pass a
\ccode{V=1} option (V for ``verbose'') to \ccode{make}, as in:

\user{make V=1}


\subsection{What gets installed by \prog{make install}, and where?}

HMMER's \prog{make install} generally follows the GNU Coding Standards and
the Filesystem Hierarchy Standard. The top-level Makefile has
variables that specify five directories where \ccode{make install}
will install things:

\vspace{1em}
\begin{tabular}{ll}
Variable             & What                 \\ \hline
\ccode{bindir}       & All HMMER programs   \\
\ccode{libdir}       & \ccode{libhmmer.a}   \\
\ccode{includedir}   & \ccode{hmmer.h}      \\
\ccode{man1dir}      & All HMMER man pages  \\
\ccode{pdfdir}       & \ccode{Userguide.pdf}\\ \hline
\end{tabular}
\vspace{1em}

These variables are constructed from some other variables, in
accordance with the GNU Coding Standards.  All of these variables are
at the top of the top-level Makefile.  Their defaults are as follows:

\vspace{1em}
\begin{tabular}{ll}
Variable              & Default                     \\ \hline
\ccode{prefix}        & \ccode{/usr/local}          \\
\ccode{exec\_prefix}  & \ccode{\${prefix}}          \\
\ccode{bindir}        & \ccode{\${exec\_prefix}/bin}\\
\ccode{libdir}        & \ccode{\${exec\_prefix}/lib}\\
\ccode{includedir}    & \ccode{\${prefix}/include}  \\
\ccode{datarootdir}   & \ccode{\${prefix}/share}    \\
\ccode{mandir}        & \ccode{\${datarootdir}/man} \\
\ccode{man1dir}       & \ccode{\${mandir}/man1}     \\ \hline
\end{tabular}
\vspace{1em}

The best way to change these defaults is when you use
\ccode{./configure}, and the most important variable to consider
changing is \ccode{--prefix}. For example, if you want to install
HMMER in a directory hierarchy all of its own, you might want to do
something like:

\user{./configure --prefix /usr/local/hmmer}

That would keep HMMER out of your system-wide directories like
\ccode{/usr/local/bin}, which might be desirable. Of course, if you do
it that way, you'd also want to add \ccode{/usr/local/hmmer/bin} to
your \ccode{\$PATH}, \ccode{/usr/local/hmmer/share/man} to your
\ccode{\$MANPATH}, etc.

These variables only affect \ccode{make install}. HMMER executables
have no pathnames compiled into them.

\subsection{Staged installations in a buildroot, for a packaging system}

HMMER's \ccode{make install} supports staged installations, accepting
the traditional \ccode{DESTDIR} variable that packagers use to specify
a buildroot. For example, you can do:

\user{make DESTDIR=/rpm/tmp/buildroot install}





\subsection{Workarounds for some unusual configure/compilation problems}

\paragraph{Configuration or compilation fails when trying to use a
  separate build directory.}  If you try to build in a build tree
(other than the source tree) and you have any trouble in configuration
or compilation, try just building in the source tree instead. Some
\ccode{make} versions don't support the VPATH mechanism needed to use
separate build trees. Another workaround is to install GNU make.


\paragraph{Configuration fails, complaining ``no acceptable grep could
  be found''.} We've seen this happen on our Sun Sparc/Solaris
machine. It's a known issue in GNU autoconf. You can either install
GNU grep, or you can insist to \ccode{./configure} that the Solaris
grep (or whatever grep you have) is ok by explicitly setting
\ccode{GREP} to a path to one that works:

\user{./configure GREP=/usr/xpg4/bin/grep}

\paragraph{Configuration warns that it's using the ``dummy''
implementation and H3 is going to be very slow.} This is what you get
if your system has a processor that we don't yet support with a fast
vector-parallel implementation. We currently support Intel/AMD
compatible processors and bigendian PowerPC compatible processors.  H3
will revert to a portable but slow version on other processors.

\paragraph{Many `make check' tests fail.} We have one report of a
system that failed to link multithread-capable system C libraries
correctly, and instead linked to one or more serial-only
libraries.\footnote{If you're a pro: the telltale phenotype of this
  failure is to configure with debugging flags on and recompile, run
  one of the failed unit test drivers (such as
  \ccode{easel/easel\_utest}) yourself and let it dump core; and use a
  debugger to examine the stack trace in the core. If it's failed in
  \ccode{\_\_errno\_location()}, it's linked a non-thread-capable
  system C library.} We were unable to reproduce the problem, and are
not sure what could possibly cause it. We optimistically believe it's
a messed-up system, not our fault, but then we often say things like that. If it does happen, it screws all
kinds of things up with the multithreaded implementation. A workaround
is to shut threading off:

\user{./configure --disable-threads}

This will compile code won't parallelize across multiple cores, of
course, but it will still work fine on a single processor at a time
(and MPI, if you build with MPI enabled).

